# ![Digital Futures](https://github.com/digital-futures-academy/DataScienceMasterResources/blob/main/Resources/datascience-notebook-header.png?raw=true)

## Extracting Data from Streams

Access to real-time data streams is a common requirement in many data-based projects.  This could be data from social media, financial markets, or IoT devices.

---

## Learner Stories

```txt
As a DATA ENGINEER,
I want to understand the fundamental concepts of data streaming,
so that I can effectively process and manage real-time data streams in my projects.

As a DATA ENGINEER,
I want to understand the principles of event-driven architecture,
so that I can design and implement systems that efficiently handle real-time events and data streams.

As a DATA ENGINEER,
I want to understand the differences between stream processing and batch processing,
so that I can choose the appropriate processing method for different use cases.

As a DATA ENGINEER,
I want to understand the principles of real-time data processing,
so that I can process and analyze data as it arrives to make timely decisions.
```

---

## Key Concepts

- **Data Streaming**: The continuous flow of data from a source to a destination.
- **Event-Driven Architecture**: A software architecture that promotes the production, detection, consumption of, and reaction to events.
- **Stream Processing**: The processing of data in motion, as it is generated or received.
- **Batch Processing**: The processing of data in large volumes, typically at scheduled intervals.
- **Real-Time Data Processing**: The processing of data as it arrives, enabling timely analysis and decision-making.

## Key Technologies

- **Apache Kafka**: A distributed event streaming platform.
- **Amazon Kinesis**: A platform for streaming data on AWS.
- **Google Cloud Pub/Sub**: A messaging service for event-driven systems.
- **Azure Event Hubs**: A scalable event processing service.
- **RabbitMQ**: An open-source message broker.

---

## 1. Data Streaming Fundamentals

Data streaming is the continuous flow of data from a source to a destination. It is used in various applications, such as real-time analytics, monitoring, and IoT. Data streams can be processed in real-time or stored for later analysis.

### Key Data Streaming Concepts

- **Data Producer**: The source of data that generates events or messages.
- **Data Consumer**: The destination of data that processes or stores events.
- **Data Stream**: A sequence of data records or events.
- **Data Pipeline**: A series of processing steps that transform data streams.
- **Data Ingestion**: The process of collecting and importing data into a system.
- **Data Processing**: The manipulation and analysis of data streams.
- **Data Storage**: The persistence of data streams for future use.

### Data Streaming Use Cases

- **Real-Time Analytics**: Processing data as it arrives to gain insights.
- **IoT Data Processing**: Handling data from sensors and devices.
- **Log Monitoring**: Analysing logs and events in real-time.
- **Fraud Detection**: Detecting anomalies and fraud patterns.
- **Recommendation Systems**: Providing real-time recommendations.
- **Social Media Analysis**: Monitoring and analysing social media data.
- **Financial Market Data**: Processing stock market data in real-time.
- **Healthcare Monitoring**: Tracking patient data and health metrics.
- **Supply Chain Management**: Monitoring inventory and logistics data.
- **Smart Cities**: Analysing data from urban sensors and devices.
- **Predictive Maintenance**: Monitoring equipment and predicting failures.
- **Security Monitoring**: Detecting security threats and breaches.

And many, many more...

---

#### Data Streams

Data streams are continuous flows of data from a source to a destination. They can be unbounded (infinite) or bounded (finite) in nature. Data streams can be processed in real-time or stored for later analysis.  Data streams can be generated by various sources, such as sensors, applications, databases, and external services.  Data streams can be processed using stream processing frameworks, such as Apache Kafka, Apache Flink, and Apache Storm.

#### Data Producers

***Data Producers*** are the sources of data that generate events or messages. They can be devices, applications, databases, or external services. They publish data to data streams, which can be consumed by data consumers and can generate events based on triggers, schedules, or external stimuli. ***Data Producers*** can be designed to scale horizontally to handle large volumes of data.

#### Data Consumers

***Data Consumers*** are the destinations of data that process or store events. They can be applications, databases, analytics platforms, or external services. They subscribe to data streams to receive and process data in real-time. ***Data Consumers*** can be designed to scale horizontally to handle large volumes of data.

#### Data Pipeline

A ***Data Pipeline*** is a series of processing steps that transform data streams from source to destination. It can include data ingestion, data processing, data enrichment, data storage, and data visualization. ***Data Pipelines*** can be designed to handle complex data processing tasks, such as real-time analytics, machine learning, and predictive modelling.

#### Data Ingestion

***Data Ingestion*** is the process of collecting and importing data into a system. It involves capturing data from various sources, transforming it into a common format, and loading it into a data store or data stream. ***Data Ingestion*** can be done in real-time or batch mode, depending on the requirements of the system.

#### Data Processing

***Data Processing*** is the manipulation and analysis of data streams. It involves transforming, filtering, aggregating, and enriching data to extract insights and generate value. ***Data Processing*** can be done in real-time or batch mode, using stream processing frameworks, such as Apache Kafka, Apache Flink, and Apache Storm.  Amazon Kinesis, Google Cloud Pub/Sub, and Azure Event Hubs are popular cloud-based stream processing services.

#### Data Storage

***Data Storage*** is the persistence of data streams for future use. It involves storing data in databases, data lakes, data warehouses, or object stores. ***Data Storage*** can be done in real-time or batch mode, depending on the requirements of the system.  Data storage solutions can be designed to scale horizontally to handle large volumes of data.  It can be in the form of structured, semi-structured, or unstructured data held in different forms such as tables, documents, key-value pairs, or graphs.

---

## 2. Event-Driven Architecture

Event-Driven Architecture (EDA) is a software architecture that promotes the production, detection, consumption of, and reaction to events. It enables decoupled, scalable, and resilient systems that can handle real-time events and data streams effectively.  Event-Driven Architecture is based on the principles of event-driven programming, which focuses on the generation, propagation, and handling of events in software systems.

### Key Event-Driven Architecture Concepts

- **Event**: A significant occurrence or change in a system that is of interest to other components.
- **Event-Driven Programming**: A programming paradigm that focuses on events and event handlers.
- **Event Producer**: A component that generates events and publishes them to event streams.
- **Event Consumer**: A component that subscribes to event streams and processes events.
- **Event Stream**: A sequence of events that are produced and consumed by components.
- **Event Broker**: A messaging system that facilitates the exchange of events between components.
- **Event Handler**: A component that reacts to events by executing specific actions or logic.

### Event-Driven Architecture Benefits

- **Decoupling**: Components are loosely coupled and communicate through events.
- **Scalability**: Systems can scale horizontally by adding more event consumers.
- **Resilience**: Systems are fault-tolerant and can recover from failures.
- **Real-Time Processing**: Events are processed in real-time, enabling timely actions.

### Event-Driven Architecture Use Cases

- **Microservices**: Communicating between microservices using events.
- **IoT Systems**: Handling sensor data and device events.
- **Real-Time Analytics**: Processing streaming data for insights.
- **Workflow Automation**: Orchestrating business processes with events.

---

#### Event-Driven Architecture

Event-Driven Architecture (EDA) is a software architecture that promotes the production, detection, consumption of, and reaction to events. It enables decoupled, scalable, and resilient systems that can handle real-time events and data streams effectively.

#### Event-Driven Programming

Event-Driven Programming is a programming paradigm that focuses on events and event handlers. It is based on the principles of event-driven architecture, which emphasize the generation, propagation, and handling of events in software systems.  Event-Driven Programming is commonly used in user interfaces, web applications, and distributed systems.  It enables asynchronous communication between components and supports real-time interactions.

#### Event Producer

An ***Event Producer*** is a component that generates events and publishes them to event streams. It can be a device, application, service, or external system that produces events based on triggers, schedules, or external stimuli. ***Event Producers*** can be designed to scale horizontally to handle large volumes of events.

#### Event Consumer

An ***Event Consumer*** is a component that subscribes to event streams and processes events. It can be an application, service, analytics platform, or external system that consumes events in real-time. ***Event Consumers*** can be designed to scale horizontally to handle large volumes of events.

#### Event Stream

An ***Event Stream*** is a sequence of events that are produced and consumed by components. It can be unbounded (infinite) or bounded (finite) in nature. ***Event Streams*** can be processed in real-time or stored for later analysis.  Event streams can be generated by various sources, such as sensors, applications, databases, and external services.

#### Event Broker

An ***Event Broker*** is a messaging system that facilitates the exchange of events between components. It acts as a central hub for event producers and consumers to communicate with each other. ***Event Brokers*** can provide features such as message queuing, message routing, message filtering, and message persistence.  Apache Kafka, Amazon Kinesis, Google Cloud Pub/Sub, and Azure Event Hubs are popular event brokers.

#### Event Handler

An ***Event Handler*** is a component that reacts to events by executing specific actions or logic. It can be a function, method, service, or workflow that processes events and triggers subsequent actions. ***Event Handlers*** can be designed to handle different types of events and perform various tasks, such as data processing, data enrichment, and data visualization.

---

## 3. Stream Processing vs. Batch Processing

***Stream Processing*** and ***Batch Processing*** are two common methods for processing data in data engineering and data science projects. They have different characteristics, use cases, and trade-offs.  Stream Processing is used for real-time data processing, while Batch Processing is used for processing large volumes of data at scheduled intervals.

### Stream Processing

- **Real-Time Processing**: Processing data as it arrives to make timely decisions.
- **Low Latency**: Minimal delay between data generation and processing.
- **Event-Driven**: Reacting to events and data streams in real-time.
- **Scalable**: Handling high volumes of data with horizontal scaling.
- **Complex Event Processing**: Analysing patterns and correlations in data streams.

#### Stream Processing Use Cases

- **Real-Time Analytics**: Processing streaming data for insights.
- **Fraud Detection**: Detecting anomalies and fraud patterns in real-time.
- **IoT Data Processing**: Handling sensor data and device events.
- **Log Monitoring**: Analysing logs and events in real-time.
- **Recommendation Systems**: Providing real-time recommendations.

### Batch Processing

- **Large Volumes**: Processing data in large batches at scheduled intervals.
- **High Throughput**: Handling high volumes of data efficiently.

#### Batch Processing Use Cases

- **Data Warehousing**: Storing and analysing historical data.
- **ETL Processing**: Extracting, transforming, and loading data into data warehouses.
- **Machine Learning Training**: Training machine learning models on historical data.
- **Report Generation**: Generating reports from large datasets.
- **Data Aggregation**: Aggregating data from multiple sources.

---

## 4. Real-Time Data Processing

Real-Time Data Processing is the processing of data as it arrives, enabling timely analysis and decision-making. It is used in various applications, such as real-time analytics, monitoring, and IoT. Real-Time Data Processing requires stream processing frameworks, event-driven architecture, and scalable data storage solutions.

### Key Real-Time Data Processing Concepts

- **Data Ingestion**: Collecting and importing data into a system in real-time.
- **Data Processing**: Manipulating and analysing data streams as they arrive.
- **Data Enrichment**: Enhancing data streams with additional information.
- **Data Visualization**: Presenting real-time insights and trends.
- **Data Storage**: Persisting data streams for future use.
- **Data Monitoring**: Monitoring data streams for anomalies and patterns.

### Real-Time Data Processing Use Cases

- **Real-Time Analytics**: Processing streaming data for insights.
- **Fraud Detection**: Detecting anomalies and fraud patterns in real-time.
- **IoT Data Processing**: Handling sensor data and device events.
- **Log Monitoring**: Analysing logs and events in real-time.
- **Recommendation Systems**: Providing real-time recommendations.

---

## Summary

- ***Data streaming*** is the continuous flow of data from a source to a destination, used in various applications such as real-time analytics, monitoring, and IoT.
- ***Event-Driven Architecture*** promotes the production, detection, consumption of, and reaction to events, enabling decoupled, scalable, and resilient systems.
- ***Stream Processing*** and ***Batch Processing*** are two common methods for processing data.
  - ***Stream Processing*** used for real-time data processing.
  - ***Batch Processing*** used for processing large volumes of data at scheduled intervals.
- ***Real-Time Data Processing*** is the processing of data as it arrives, enabling timely analysis and decision-making in applications such as real-time analytics, monitoring, and IoT.
